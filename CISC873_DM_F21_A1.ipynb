{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkWXSXvmG7JM4gvhBYwXtV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-zeng/python_projects/blob/main/CISC873_DM_F21_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi7SyDxgEfjv"
      },
      "source": [
        "[Wish.com Product Rating Prediction](https://www.kaggle.com/c/cisc873-dm-f21-a1/overview)\n",
        "\n",
        "Student: Vinson Zeng\n",
        "\n",
        "Student #: 05550960"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJVHrGSZEeDx"
      },
      "source": [
        "**Data Science meme**\n",
        "\n",
        "![dataScienceMeme.jpg](data:image/jpeg;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiXVPr14KzQ_"
      },
      "source": [
        "**Problem Formulation**\n",
        "\n",
        "**Define the problem:** Accurately predict product ratings based on known features to understand the customer base of wish.com.\n",
        "\n",
        "**What is the input? What is the output?:** Input are features known for a product (independent variables), output is prediction of customer rating (dependent variable).\n",
        "\n",
        "**What data mining function is required?:** Classification.\n",
        "\n",
        "**What could be the challenges?:** One challenge is to effectively perform data preparation through data cleaning, feature selection, and data transformation. Another challenge is to evaluate the classification methods based on metrics, such as F1 score, and hyperparameter tuning.\n",
        "\n",
        "**What is the impact?:** If the data is not effectively prepared the model can overfit or underfit. Overfitting can occur due to the model following the noise, or errors, too closely. Underfitting occurs when the model does performs poorly on training data as it is unable to effectively learn the relationship between the independent and dependent variables. Both overfitting and underfitting will result in poor generalization of the model and thus it will result in poor prediction.\n",
        "\n",
        "**What is an ideal solution?:** An ideal solution would be an F1 score of 1, which indicates a perfect model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO1a6Qrrt73w"
      },
      "source": [
        "**Understand the Template**\n",
        "\n",
        "**What is the experimental protocol used and how was it carried out?**\n",
        "\n",
        "The goal is to predict product ratings based on the known features for those products. We can estimate how likely people will like a new product without listing it with by building a classification model. The column 'rating' is used as the dependent variable and the remaining features are independent variables.\n",
        "\n",
        "Product data is imported from wish.com and preprocessed. The data is then split into training and validation sets in a 70/30 partition before encoding categorical features. Validation data is then transformed using mappings built from the training set.\n",
        "\n",
        "The model is trained using the training data and then predicts the 'rating' for the validation data set on the basis of the trained model.\n",
        "\n",
        "**What preprocessing steps are used in the template?**\n",
        "\n",
        "Data filtering, random sampling without replacement, imputation, feature selection, data transformation (encoding).\n",
        "\n",
        "**What is the model being used?**\n",
        "\n",
        "Logistic regression model is being used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQzXAeOF5v1z"
      },
      "source": [
        "**Post-trials Thoughts (see below for trials and their observations):**\n",
        "\n",
        "Increasing the number of trees in the forest for Trial#9 did not appear to improve the performance of the model. After ten trials, I believe that there could be some improvement in the data preprocessing step or better tuning of hyperparameters. The largest improvements I observed to the F1 score came from scaling the features and using a random forest classifier.\n",
        "\n",
        "From the beginning I have done the following:\n",
        "\n",
        "*   Impute missing values using mode\n",
        "*   Normalize features using StandardScaler\n",
        "*   Tune logistic regression with max_iter=500\n",
        "*   Used a correlation matrix to drop features I thought may be unnecessary\n",
        "*   Tune logistic regression with class_weight='balanced'\n",
        "*   Tune logistic regression with multi_class='multinomial'\n",
        "*   Used a decision tree classifier\n",
        "*   Used a SVM classifier\n",
        "*   Used a random forest classifier\n",
        "*   Tune the random forest model with n_estimators=500\n",
        "\n",
        "\n",
        "*   Feature selection: removed 'currency_buyer' 'product_variation_size_id', 'shipping_option_name', 'merchant_title', 'merchant_name', 'merchant_info_subtitle', 'theme', 'crawl_month'.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVgAEAJY6qXV"
      },
      "source": [
        "**Answer the questions below (briefly):**\n",
        "\n",
        "ðŸŒˆ Why Data Mining is a misnomer? What is another preferred name?\n",
        "\n",
        "The goal of data mining is pattern extraction and gaining insight or knowledge from data. There is no actual extraction of data itself. Machine learning, data science, and/or data analysis are examples of better names.\n",
        "\n",
        "ðŸŒˆ What is the general knowledge discovery process? What is the difference\n",
        "between a data engineer and data scientist/AI engineer?\n",
        "\n",
        "The general knowledge discovery process is dat cleaning, dat integration, data selection, data transformation, data mining, pattern evaluation, and knowledge presentation. A data engineer is involved in developing, testing, and maintaining architectures such as databases. A data scientist cleans, organizes, and analyzes data.\n",
        "\n",
        "ðŸŒˆ In data mining, what is the difference between prediction and categorization?\n",
        "\n",
        "Prediction is identifying some unknown class labels, while categorization is identifying the class label for a new observation.\n",
        "\n",
        "ðŸŒˆ In a linear model, which regularization method encourages sparsity?\n",
        "\n",
        "L-1 Regularization (Lasso) encourages model sparsity.\n",
        "\n",
        "ðŸŒˆ Why we need GD for optimization, rather than simply use linear algebra to solve a simple linear model?\n",
        "\n",
        "Gradient descent is computationally faster and cheaper for linear regression.\n",
        "\n",
        "ðŸŒˆ In terms of bias and variance, defines what is overfitting and under fitting?\n",
        "\n",
        "Overfitting: high variance and low bias\n",
        "\n",
        "Underfitting: low variance and high bias\n",
        "\n",
        "ðŸŒˆ Why data science/machine learning is a bad idea in the context of information security?\n",
        "\n",
        "There are increased risks of data breaches, increased uncertainty, difficulty in evaluating changes in the AI, difficulty in verifying compliance. There are also issues of responsibility, accountability, and liability when using machine learning for information security.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5ZYdK_RGm4B"
      },
      "source": [
        "**Template**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkFf8zU5JZ0L"
      },
      "source": [
        "# import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnVMYtKXJZ0P"
      },
      "source": [
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "data = data.fillna(0) # replace NA/NaN values with 0\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_jrZ2WnJZ0Q"
      },
      "source": [
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGT6nZ31JZ0R"
      },
      "source": [
        "tr # training data preview"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISRM71_UJZ0S"
      },
      "source": [
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "\n",
        "\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "tr # training data preview, categorical features are now encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PxTLIgfJZ0T"
      },
      "source": [
        "print('categorical features')\n",
        "pprint(list(dict_cat.keys())) # formatted printing of categorical features list (the keys in dict_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhLZqHsxJZ0T"
      },
      "source": [
        "print('Lets see what the mapping for column origin_country :')\n",
        "pprint(dict_cat['origin_country']) # print mapping for column, pprint formats output from smallest to largest integer\n",
        "print('It is a string to integer mapping')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZGB7B7mJZ0U"
      },
      "source": [
        "# then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs7NWDp6JZ0U"
      },
      "source": [
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "clf = LogisticRegression().fit(tr_x, tr_y) # train model using training data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9NLkqrAJZ0W"
      },
      "source": [
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWWKjhkAJZ0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3440d74e-4b70-4107-bd5d-3e3de46b351d"
      },
      "source": [
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.740983606557377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGGVmQCFJZ0Y"
      },
      "source": [
        "# once you are happy with your local model, let's prepare a submission\n",
        "# we need to apply the same preprocessing steps on the testing set as you did before you train the model\n",
        "\n",
        "test_data = pd.read_csv('test_new.csv').sample(frac=1) # sample all rows without replacement\n",
        "_id = test_data['id']\n",
        "test_data = test_data.fillna(0) # fill NA/NaN values with 0\n",
        "test_data = test_data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns\n",
        "test_data[cat_cols] = test_data[cat_cols].apply(lambda col: col.map(dict_cat[col.name])) # transform test set with mappings from training set\n",
        "\n",
        "# again, not-seen string value filled with -1\n",
        "test_data = test_data.fillna(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCAR-XXlJZ0Y"
      },
      "source": [
        "pred_test = clf.predict(test_data) # predict for test data based on trained model\n",
        "pred_df = pd.DataFrame(data={'id': np.asarray(_id), 'rating': pred_test}) # predictions for 'id' as data frame\n",
        "pred_df.to_csv('pred_walkthrough.csv', index=False) # save predictions to csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zktj1grqbBnb"
      },
      "source": [
        "**Trial#0**\n",
        "\n",
        "This trial I will impute missing values. Missing values for a numerical feature will be replaced with its mean, while categorical features will use the mode. I think providing more data may improve the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qExKZm_rpUf0"
      },
      "source": [
        "# Trial#0\n",
        "\n",
        "#import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwSqtnYcpWsU"
      },
      "source": [
        "#load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXnBsaIMqqM9",
        "outputId": "2dfec317-429d-4a1d-a79c-8a92beaf4b23"
      },
      "source": [
        "# imputation\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "data.isna().sum() # check missing values again"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "price                           0\n",
              "retail_price                    0\n",
              "currency_buyer                  0\n",
              "units_sold                      0\n",
              "uses_ad_boosts                  0\n",
              "rating                          0\n",
              "rating_count                    0\n",
              "badges_count                    0\n",
              "badge_local_product             0\n",
              "badge_product_quality           0\n",
              "badge_fast_shipping             0\n",
              "product_color                   0\n",
              "product_variation_size_id       0\n",
              "product_variation_inventory     0\n",
              "shipping_option_name            0\n",
              "shipping_option_price           0\n",
              "shipping_is_express             0\n",
              "countries_shipped_to            0\n",
              "inventory_total                 0\n",
              "has_urgency_banner              0\n",
              "urgency_text                    0\n",
              "origin_country                  0\n",
              "merchant_title                  0\n",
              "merchant_name                   0\n",
              "merchant_info_subtitle          0\n",
              "merchant_rating_count           0\n",
              "merchant_rating                 0\n",
              "merchant_has_profile_picture    0\n",
              "theme                           0\n",
              "crawl_month                     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 489
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYeQxO2Uo49Y",
        "outputId": "63a81bcb-b480-4997-c893-db33a7f3faf9"
      },
      "source": [
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set\n",
        "\n",
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "\n",
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "clf = LogisticRegression().fit(tr_x, tr_y) # train model using training data\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6888888888888889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UGDnY1QuZmu"
      },
      "source": [
        "**Trial#1**\n",
        "\n",
        "The F1 score from trial#0 appears to be similar to the template score. There is a large difference in magnitude between some attributes. For example, one merchant rating count is over 200,000. This is several magnitudes larger than many other values in various attributes. I will normalize the features using StandardScaler so that they may contribute equally to the model fitting, which reduces bias. I think this may improve the F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YXt9CMFv60-"
      },
      "source": [
        "# Trial#1\n",
        "### import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler # import for scaling\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzyDqpwuv-IA"
      },
      "source": [
        "### load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "\n",
        "# replace missing values with mode of respective columns\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeUBlGSR4OzQ"
      },
      "source": [
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhtyzM6xv8jl",
        "outputId": "a5834d87-d160-4592-9c7a-16fa41ed5c57"
      },
      "source": [
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0JATgGmvPyi",
        "outputId": "5d379b52-ae0d-455f-cd7b-fdbeb6b3f488"
      },
      "source": [
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "\n",
        "# apply scalar transformation to features before training model\n",
        "scaler = StandardScaler() # create object of StandardScaler() function\n",
        "tr_x = scaler.fit_transform(tr_x) # fit scaler on training data predictors and transform\n",
        "\n",
        "clf = LogisticRegression().fit(tr_x, tr_y) # train model using training data\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "val_x = scaler.transform(val_x) # standardize validation using fit from training predictors\n",
        "\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7296511627906975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMpkM8vJzfBo"
      },
      "source": [
        "**Trial#2**\n",
        "\n",
        "After scaling in trial#1, the F1 score appears to have improved to approximately 0.7 to 0.75. Whereas before the scores would be approximately 0.65 to 0.75.\n",
        "\n",
        "In this trial I will increase max_iter to an abitrary number of 500 so the solver can converge. I think it may increase the performance of the model since it may find the optimal solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce8vOHsV8oeg",
        "outputId": "fc0a1715-8a46-41b1-dbd1-0d849018e5a5"
      },
      "source": [
        "# Trial#2\n",
        "### import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler # import for scaling\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "### load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "\n",
        "# replace missing values with mode of respective columns\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns\n",
        "\n",
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set\n",
        "\n",
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "\n",
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "\n",
        "# apply scalar transformation to features before training model\n",
        "scaler = StandardScaler() # create object of StandardScaler() function\n",
        "tr_x = scaler.fit_transform(tr_x) # fit scaler on training data predictors and transform\n",
        "\n",
        "clf = LogisticRegression(max_iter=500).fit(tr_x, tr_y) # train model using training data, increase max_iter from default of 100\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "val_x = scaler.transform(val_x) # standardize validation using fit from training predictors\n",
        "\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7283950617283951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-ujdjUxLmQy"
      },
      "source": [
        "**Trial#3**\n",
        "\n",
        "Increasing the max_iter in trial#2 finally allowed the solver to converge. I ran it several times and it seems to result in a better F1 score, which appears to be more consistently around 0.72.\n",
        "\n",
        "This trial I will use a correlation matrix to look for correlated features. I will decide to drop correlated features so that the model accuracy may improve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3uwsbqXLVLZ"
      },
      "source": [
        "# Trial#3\n",
        "### import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler # import for scaling\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "### load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "\n",
        "# replace missing values with mode of respective columns\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W9p0jsKLdqd"
      },
      "source": [
        "# make correlation matrix\n",
        "corrMatrix = data.corr()\n",
        "\n",
        "#mask redundant squares\n",
        "mask = np.triu(np.ones_like(corrMatrix, dtype=bool))\n",
        "\n",
        "#adjust dataframe and mask\n",
        "mask = mask[1:, :-1]\n",
        "corrDF = corrMatrix.iloc[1:,:-1].copy()\n",
        "fig, ax = plt.subplots(figsize=(20,20))\n",
        "\n",
        "#colour for heatmap\n",
        "cmap = sns.diverging_palette(220,20,as_cmap=True)\n",
        "#heatmap\n",
        "\n",
        "sns.heatmap(\n",
        "    corrDF,\n",
        "    mask=mask,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    linewidths=5,\n",
        "    cmap=cmap,\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        "    cbar_kws={\"shrink\": .8},\n",
        "    square=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIOCqDsJkfa",
        "outputId": "5b06072d-93a5-4c32-fb9f-00a12c39b6b9"
      },
      "source": [
        "data = data.drop(['merchant_id',\n",
        "                  'merchant_profile_picture',\n",
        "                  'id',\n",
        "                  'tags',\n",
        "                  'shipping_option_price',\n",
        "                  'units_sold',\n",
        "                  'badges_count',\n",
        "                  'badge_fast_shipping',\n",
        "                  'merchant_has_profile_picture'], axis=1) # drop specified columns\n",
        "\n",
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set\n",
        "\n",
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "\n",
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "\n",
        "# apply scalar transformation to features before training model\n",
        "scaler = StandardScaler() # create object of StandardScaler() function\n",
        "tr_x = scaler.fit_transform(tr_x) # fit scaler on training data predictors and transform\n",
        "\n",
        "clf = LogisticRegression(max_iter=500).fit(tr_x, tr_y) # train model using training data, increase max_iter from default of 100\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "val_x = scaler.transform(val_x) # standardize validation using fit from training predictors\n",
        "\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7138643067846607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIhuQ8qlQHUu"
      },
      "source": [
        "**Trial#4**\n",
        "\n",
        "Removing 'shipping_option_price', 'units_sold', 'badges_count', 'badge_fast_shipping', and 'merchant_has_profile_picture' did not appear to have an effect on the F1 score. I removed those either due to high correlation (0.8 or greater) or because I thought they were not useful in predicting the rating. However, After the previous trial, I will keep them in case I have incorrectly assumed anything and because the model performance did not seem to change.\n",
        "\n",
        "For this trial I will set the hyperparameter 'class_weight' to 'balanced' since there appears to be a large number of 4.0 ratings in comparison to the rest. I think it will have some sort of effect on the model performance. I hope it will increase the performance, but I am not entirely sure so I will test it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faKnIlnzQGjv",
        "outputId": "c3ca7837-a8a3-45f2-c28e-e31391b2363a"
      },
      "source": [
        "# Trial#4\n",
        "### import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler # import for scaling\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "### load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "\n",
        "# replace missing values with mode of respective columns\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns\n",
        "\n",
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set\n",
        "\n",
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "\n",
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "\n",
        "# apply scalar transformation to features before training model\n",
        "scaler = StandardScaler() # create object of StandardScaler() function\n",
        "tr_x = scaler.fit_transform(tr_x) # fit scaler on training data predictors and transform\n",
        "\n",
        "clf = LogisticRegression(max_iter=500, class_weight='balanced').fit(tr_x, tr_y) # train model using training data use class_weight of 'balanced'\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "val_x = scaler.transform(val_x) # standardize validation using fit from training predictors\n",
        "\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5184135977337111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l_2jeoHSVgT"
      },
      "source": [
        "**Trial#5**\n",
        "\n",
        "Trial#4 did not go as I had hoped as the F1 score is much lower (approximately 0.44). I will leave the class_weight as the default and try setting the hyperparameter 'multi_class' to 'multinomial' to see if cross-entropy loss will yield better performance from the model. I predict it will increase the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnjQMIdBSgKH",
        "outputId": "99bf3906-0abe-47c2-ea7a-c435a1015d7d"
      },
      "source": [
        "# Trial#5\n",
        "### import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler # import for scaling\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "### load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "\n",
        "# replace missing values with mode of respective columns\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns\n",
        "\n",
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set\n",
        "\n",
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "\n",
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "\n",
        "# apply scalar transformation to features before training model\n",
        "scaler = StandardScaler() # create object of StandardScaler() function\n",
        "tr_x = scaler.fit_transform(tr_x) # fit scaler on training data predictors and transform\n",
        "\n",
        "clf = LogisticRegression(max_iter=500,multi_class='multinomial').fit(tr_x, tr_y) # train model using training data\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "val_x = scaler.transform(val_x) # standardize validation using fit from training predictors\n",
        "\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7438271604938271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECO1mBulcDv7"
      },
      "source": [
        "**Trial#6**\n",
        "\n",
        "Trial#5 resulted in F1 scores similar to the default 'ovr' scheme for the multi_class parameter, which are approximately in the vicinity of 0.72.\n",
        "\n",
        "After several trials with logistic regression I will attempt to use the decision tree model. I have been unable to improve the F1 score beyond the average of approximately 0.72. For a multinomial classification problem the decision tree may be able to capture the boundaries better. I predict the F1 score of the decision tree will be higher than the logistic regression model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwkOc7StlDVu",
        "outputId": "ce686afb-6231-4704-c708-678ceb2d6dd9"
      },
      "source": [
        "# Trial#6\n",
        "### import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "### load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "\n",
        "# replace missing values with mode of respective columns\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns\n",
        "\n",
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set\n",
        "\n",
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "\n",
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "\n",
        "# apply scalar transformation to features before training model\n",
        "scaler = StandardScaler() # create object of StandardScaler() function\n",
        "tr_x = scaler.fit_transform(tr_x) # fit scaler on training data predictors and transform\n",
        "\n",
        "clf = DecisionTreeClassifier().fit(tr_x, tr_y) # train model using training data\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "val_x = scaler.transform(val_x) # standardize validation using fit from training predictors\n",
        "\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7180327868852459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKN-QXNSpx9M"
      },
      "source": [
        "**Trial#7**\n",
        "\n",
        "The initial run with the decision tree in the previous trial yielded F1 scores approximately from 0.55 to 0.73, which is worse than the logistic regression model. Next I will try a SVM model since it can find the 'best' margin separating classes in comparison to logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySH7pegutKu9",
        "outputId": "82556428-0612-42cf-af16-a571e7ca4ed4"
      },
      "source": [
        "# Trial#7\n",
        "### import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "### load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "\n",
        "# replace missing values with mode of respective columns\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns\n",
        "\n",
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set\n",
        "\n",
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "\n",
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "\n",
        "# apply scalar transformation to features before training model\n",
        "scaler = StandardScaler() # create object of StandardScaler() function\n",
        "tr_x = scaler.fit_transform(tr_x) # fit scaler on training data predictors and transform\n",
        "\n",
        "clf = svm.SVC()\n",
        "clf.fit(tr_x, tr_y) # train model using training data\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "val_x = scaler.transform(val_x) # standardize validation using fit from training predictors\n",
        "\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7391304347826085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJQaQzhRyOH3"
      },
      "source": [
        "**Trial#8**\n",
        "\n",
        "The SVM model yields F1 scores in the 0.7 range. The next model I will try is the random forest model. Since it utilizes ensemble learning, I predict it will have a better F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKT8D0mszQ7Z",
        "outputId": "faa1d6dd-9d33-4c25-8670-ba95846eeab6"
      },
      "source": [
        "# Trial#8\n",
        "### import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "### load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "\n",
        "# replace missing values with mode of respective columns\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1) # drop specified columns\n",
        "\n",
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set\n",
        "\n",
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "\n",
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "\n",
        "# apply scalar transformation to features before training model\n",
        "scaler = StandardScaler() # create object of StandardScaler() function\n",
        "tr_x = scaler.fit_transform(tr_x) # fit scaler on training data predictors and transform\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(tr_x, tr_y) # train model using training data\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "val_x = scaler.transform(val_x) # standardize validation using fit from training predictors\n",
        "\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7909967845659164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcCKh2890cC2"
      },
      "source": [
        "**Trial#9**\n",
        "\n",
        "Trial#8 resulted in the best F1 scores so far, in which it ranges approximately from 0.75 to 0.82. For this 10th trial I will increase the number of trees in the forest to an arbitrary number of 500 in hopes of increasing the model performance. With more trees, I would predict that the predictive accuracy would increase. Additionally, I will drop some columns that I find to be irrelevant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQT3-Ukr0mJk",
        "outputId": "168f9afd-183a-4b05-995d-7d5ba9d90417"
      },
      "source": [
        "# Trial#9\n",
        "### import modules and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "### load and preprocess data\n",
        "data = pd.read_csv('train_new.csv').sample(frac=1) # randomly sample all rows without replacement\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])] # filter to return rows only with values specified\n",
        "\n",
        "# replace missing values with mode of respective columns\n",
        "for column in data.columns:\n",
        "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "data = data.drop(['merchant_id',\n",
        "                  'merchant_profile_picture',\n",
        "                  'id',\n",
        "                  'tags',\n",
        "                  'currency_buyer',\n",
        "                  'product_variation_size_id',\n",
        "                  'shipping_option_name',\n",
        "                  'merchant_title',\n",
        "                  'merchant_name',\n",
        "                  'merchant_info_subtitle',\n",
        "                  'theme',\n",
        "                  'crawl_month'\n",
        "                  ], axis=1) # drop specified columns\n",
        "\n",
        "### split data into training and validation sets\n",
        "msk = np.random.rand(len(data)) < 0.7 # return boolean array with 70% of True values\n",
        "tr = data[msk] # partition approximately 70% of data into training set\n",
        "val = data[~msk] # flip True and False with ~msk to partition approximately 30% of data into validation set\n",
        "\n",
        "### encode categorical features\n",
        "dict_cat = {} # create empty dictionary\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list() # create list of categorical columns\n",
        "\n",
        "# encoding function for categorical features\n",
        "def cat_digit(col):\n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes # categorical encoding of column\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded))) # column as key and categorical encoding as values\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col)) # feed each element of cat_cols into cat_digit function\n",
        "\n",
        "### then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)\n",
        "\n",
        "### train model\n",
        "tr_y = tr['rating'] # 'rating' as dependent variable\n",
        "tr_x = tr.drop('rating', axis=1) # drop 'rating from columns, remainder as independent variables\n",
        "\n",
        "# apply scalar transformation to features before training model\n",
        "scaler = StandardScaler() # create object of StandardScaler() function\n",
        "tr_x = scaler.fit_transform(tr_x) # fit scaler on training data predictors and transform\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=500) # tune n_estimators parameter\n",
        "clf.fit(tr_x, tr_y) # train model using training data\n",
        "\n",
        "### use trained model on validation data\n",
        "val_y = val['rating'] # set dependent variable for validation data\n",
        "val_x = val.drop('rating', axis=1) # set independent variables for validation data\n",
        "val_x = scaler.transform(val_x) # standardize validation using fit from training predictors\n",
        "\n",
        "pred_val = clf.predict(val_x) # predict 'rating' for validation data on basis of trained model\n",
        "\n",
        "### compute F1 score\n",
        "val_score = f1_score(val_y, pred_val, average='micro') # compute F1 score with 'micro' weighting, which uses global true positives, false negatives, and false positives\n",
        "print(val_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8066465256797583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i71XBLW5_0_H"
      },
      "source": [
        "**Test Submission**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx_iDZGh__oG"
      },
      "source": [
        "# once you are happy with your local model, let's prepare a submission\n",
        "# we need to apply the same preprocessing steps on the testing set as you did before you train the model\n",
        "\n",
        "test_data = pd.read_csv('test_new.csv').sample(frac=1) # sample all rows without replacement\n",
        "_id = test_data['id']\n",
        "# replace missing values in columns with respective mode\n",
        "for column in test_data.columns:\n",
        "    test_data[column].fillna(test_data[column].mode()[0], inplace=True)\n",
        "\n",
        "test_data = test_data.drop(['merchant_id',\n",
        "                  'merchant_profile_picture',\n",
        "                  'id',\n",
        "                  'tags',\n",
        "                  'currency_buyer',\n",
        "                  'product_variation_size_id',\n",
        "                  'shipping_option_name',\n",
        "                  'merchant_title',\n",
        "                  'merchant_name',\n",
        "                  'merchant_info_subtitle',\n",
        "                  'theme',\n",
        "                  'crawl_month'\n",
        "                  ], axis=1) # drop specified columns\n",
        "test_data[cat_cols] = test_data[cat_cols].apply(lambda col: col.map(dict_cat[col.name])) # transform test set with mappings from training set\n",
        "\n",
        "# again, not-seen string value filled with -1\n",
        "test_data = test_data.fillna(-1)\n",
        "\n",
        "# standardize test data using fit from training data\n",
        "test_data = scaler.transform(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-smvy1E_zxJ"
      },
      "source": [
        "pred_test = clf.predict(test_data) # predict for test data based on trained model\n",
        "pred_df = pd.DataFrame(data={'id': np.asarray(_id), 'rating': pred_test}) # predictions for 'id' as data frame\n",
        "pred_df.to_csv('pred_walkthrough.csv', index=False) # save predictions to csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}